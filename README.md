지역이름 검색별 이슈 키워드 통계 :city_sunrise:
========================== 
---
## 1. 설명

```
2020 경기도 데이터 산업인력 양성 사업의 빅데이터 전문가 과정 SEMI-PROJECT-1
경기도의 28개의 시와 3개의 군을 합친 31개의 도시를 검색하여 스크래핑한 이후 어떤 키워드가 많이 사용되는지 분석
```

## 2. 스크래핑한 채널

---
### - :green_book:Naver Blog    <허지훈> 

### - :camera:Instagram      <이채은> 

### - :newspaper:Naver News   <배지연>



## 3. 사용된 라이브러리

---

```
- selenium
- BeautifulSoup 
- konlpy
- collections
- time
- pandas
- numpy
- pickle
- json
- csv
- sys
- unicodedata
```



## 4. 시각화 

---
### PiePlot
![pieplot](img/pieplot.png)


### Tree Map
![tree_map](img/tree_map.png)



### Word Cloud

![tree_map](img/word_cloud.png)

## 5. 산출물

---
### - :green_book: Naver Blog    <허지훈> 
- get_links.pickle 스크래핑해야하는 링크 dict
- 1.* { 가평군, 고양시,광명시, 광주시, 구리시, 군포시, 김포시 }
- 2.* { 남양주시, 동두천시, 부천시, 성남시, 과천시 }
- 3.* { 안양시, 양주시, 양평군, 여주시 }
- 4.* { 연천군, 오산시, 용이니, 의왕시 }
- 5.* { 의정부시, 이천시, 파주시, 평택시 }
- 6.* { 포천시, 하남시, 화성시 }
- 7.* { 수원시, 시흥시, 안산시, 안성시 }
- file_ * _word_dict.* 지역별 word_count
- naver_blog_word_dict.csv  지역별 word_count를 한파일로 모음

### - :camera: Instagram      <이채은> 

- dfcounts(insta).pickle {  '가평군',  '고양시',  '과천시', '광명시',   '광주시',  '구리시',  '군포시', '김포시',  '남양주시', '동두천시', '부천시', '성남시',  '수원시',  '시흥시',  '안산시', '안성시',  '안양시',  '양주시',  '양평군', '여주시',  '연천군',  '오산시',  '용인시', '의왕시',  '의정부시', '이천시',  '파주시', '평택시',  '포천시',  '하남시',  '화성시'}

### - :newspaper: Naver News   <배지연>

- nesw_texts.csv {  '가평군',  '고양시',  '과천시', '광명시',   '광주시',  '구리시',  '군포시', '김포시',  '남양주시', '동두천시', '부천시', '성남시',  '수원시',  '시흥시',  '안산시', '안성시',  '안양시',  '양주시',  '양평군', '여주시',  '연천군',  '오산시',  '용인시', '의왕시',  '의정부시', '이천시',  '파주시', '평택시',  '포천시',  '하남시',  '화성시' }



## 6. 작업 도중 발생한 이슈들

---
>:green_book: Naver Blog    <허지훈> 
>>1. 블로그 내용 수집중 HTML내부가 Iframe으로 되어있어서 태그를 찾지 못했음
>>>=> 해결: Iframe에있는 src 경로로 다시 접근하여 scrapping 하였음   

>>2. 네이버 블로그는 많은 Frame이 있기 때문에 모든 블로그를 수집하지 못했음
>>>=> 해결: 1개의 틀만 정하고 잡아놓은 URL만 수집하였음 

>>3. 블로그 특성상 많은 양의 텍스트가 존재하여 서버가 자주 down됨
>>>=> 해결: Colab을 이용하여 도시를 나눠 분할작업 후 Pickle을 사용하여 데이터 저장함  
>>>=> Colab은 Ubuntu운영체제로 화면이 없기 때문에 브라우저를 띄우지않고 headless상태로 브라우저에 접속해야함 -> Selenium에 headless옵션 사용

>>4. 파일 용량이 커서 csv로 Parsing하여 저장할 때 Error발생함
>>>=> 해결: import csv      import sys          csv.field_size_limit(sys.maxsize) csv에 허용가능한 필드의 사이즈를 최대로 늘림

>>5. 파일 용량이 커서 1개의 csv로 merge를 하지 못했음 (메모리초과)
>>>=> 해결: 해결하지 못하여서 7개의 파일로 분할하여 작업함



>  :camera: Instagram      <이채은>
>
> > 1. 인스타그램 크롤링 문제
> >    인스타는 크롤링 하기 까다로움. 계속 로그인을 해줘야 하고 명령이 먹히지 않을 때가 있음. 쉽게 과부화 됨.
> >
> > > => 파트를 나눠서 계속 드라이버가 다운 되는 부분은 직접 클릭해줌

> > 2. pos 테깅 텍스트길이 제한.
> >    pos를 처리해야 하는 텍스트 길이가 길지만 konlpy에서 제공하는 okt 클래스는 약 1000자까지 받음. 주피터에서는 800자 이상 돌리면 커널이 다운됨
> >
> > > =>   colab을 이용하고 for 문으로 텍스트를 800 자 단위로 나누어 pos처리 해주고 새 리스트에 append 해줌.



>:newspaper: Naver news      <배지연>
>
>>1. 데이터를 dict = {'city':[text1, text2,...]} 로 만들었는데, 시군별로  생성되는 value list가 덮어쓰기 되지 않았는지 확인이 필요했음
>>
>>>=> 해결 : 만들어진 dict에서 value list sample을 띄우거나, len[]로 item 개수를 확인함   

>>2. link에서 text를 추출해서 dict를 만든 이후로 주피터 노트북 가동속도가 느려짐
>>
>>>=> 해결 : 중간과정에서 pickle 파일로 저장하고 새로 읽어서 다시 dict를 선언하고 수정함 

>>3. class 태그로 텍스트를 얻을 수 없는 링크가 몇 개 있었음
>>
>>>=> 해결: 예외처리하여 링크를 따로 추출하고 다음 페이지로 넘김